#!/usr/bin/env python3
"""
IELTS Novel Flow - å…¨åº“è¯æ±‡è¦†ç›–ç‡æ£€æŸ¥

éœ€æ±‚ï¼š
æ£€æŸ¥â€œå·²ä¸Šæ¶çš„å…¨éƒ¨å°è¯´ï¼ˆsrc/data/generated/book-*.jsonï¼‰â€ä¸­å‡ºç°çš„è‹±æ–‡å•è¯ï¼Œ
æ˜¯å¦è¦†ç›–äº† IELTS æ ¸å¿ƒè¯åº“ï¼ˆtools/ielts_source.jsonï¼‰ã€‚

ç»Ÿè®¡å£å¾„ï¼š
- ä»æ¯æœ¬ä¹¦çš„ç« èŠ‚ JSONï¼ˆbook-*.jsonï¼‰çš„ content å­—æ®µä¸­æå– {word|meaning} çš„ word
- word ç»Ÿä¸€ lower + strip
- ä¸ ielts_source.jsonï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰åšå¯¹æ¯”

è¾“å‡ºï¼š
- æ€»ä¹¦ç±æ•°ã€æ€»æ ‡è®°æ¬¡æ•°ã€å»é‡åçš„å·²è¦†ç›–è¯æ•°
- è¦†ç›–ç‡ï¼ˆè¦†ç›–è¯æ•° / è¯åº“æ€»æ•°ï¼‰
- ç¼ºå¤±è¯æ•°é‡ï¼Œå¹¶å°†ç¼ºå¤±è¯å†™å…¥ tools/missing_ielts_words.txt
- åŒæ—¶å†™å…¥ tools/coverage_summary.jsonï¼ˆä¾¿äºç•™æ¡£ï¼‰

ç”¨æ³•ï¼š
  cd tools
  python3 check_library_vocab_coverage.py
"""

import json
import os
import re
import sys
from typing import Dict, List, Set, Tuple


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)

IELTS_SOURCE_PATH = os.path.join(BASE_DIR, "ielts_source.json")
GENERATED_DIR = os.path.join(PROJECT_ROOT, "src", "data", "generated")

OUT_MISSING_TXT = os.path.join(BASE_DIR, "missing_ielts_words.txt")
OUT_SUMMARY_JSON = os.path.join(BASE_DIR, "coverage_summary.json")


WORD_MARK_PATTERN = re.compile(r"\{([^|{}]+)\|([^}]+)\}")


def load_ielts_source_words(path: str) -> List[str]:
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° IELTS è¯åº“æ–‡ä»¶ï¼š{path}")
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, list):
        raise ValueError("ielts_source.json å¿…é¡»æ˜¯å­—ç¬¦ä¸²æ•°ç»„")
    words: List[str] = []
    for item in data:
        if isinstance(item, str):
            w = item.strip()
            if w:
                words.append(w)
    return words


def list_generated_books(dir_path: str) -> List[str]:
    if not os.path.isdir(dir_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç« èŠ‚ç”Ÿæˆç›®å½•ï¼š{dir_path}")
    files = []
    for name in os.listdir(dir_path):
        if name.startswith("book-") and name.endswith(".json"):
            files.append(os.path.join(dir_path, name))
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæ—§->æ–°ï¼‰
    files.sort(key=lambda p: os.path.getmtime(p))
    return files


def extract_words_from_content(content: str) -> Tuple[Set[str], int]:
    """
    Returns: (unique_words_in_content, total_mark_count)
    """
    unique: Set[str] = set()
    total_marks = 0
    for m in WORD_MARK_PATTERN.finditer(content):
        total_marks += 1
        w = m.group(1).strip().lower()
        if w:
            unique.add(w)
    return unique, total_marks


def load_book_content(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if not isinstance(data, dict):
        raise ValueError(f"ç« èŠ‚æ–‡ä»¶ä¸æ˜¯å¯¹è±¡ï¼š{path}")
    content = data.get("content", "")
    if not isinstance(content, str):
        raise ValueError(f"ç« èŠ‚æ–‡ä»¶ content ä¸æ˜¯å­—ç¬¦ä¸²ï¼š{path}")
    return content


def main() -> None:
    print("=" * 60)
    print("ğŸ“š å…¨åº“è¯æ±‡è¦†ç›–ç‡æ£€æŸ¥ï¼ˆå·²ä¸Šæ¶å°è¯´ vs IELTS æ ¸å¿ƒè¯åº“ï¼‰")
    print("=" * 60)

    try:
        source_words = load_ielts_source_words(IELTS_SOURCE_PATH)
        source_set = set([w.lower() for w in source_words])
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½ IELTS è¯åº“ï¼š{e}")
        sys.exit(1)

    try:
        book_files = list_generated_books(GENERATED_DIR)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–å·²ä¸Šæ¶ä¹¦ç±ï¼š{e}")
        sys.exit(1)

    if not book_files:
        print("âš ï¸  src/data/generated ä¸‹æ²¡æœ‰ book-*.jsonï¼Œå½“å‰æ²¡æœ‰å¯æ£€æŸ¥çš„ä¸Šæ¶å°è¯´ã€‚")
        sys.exit(0)

    covered_words: Set[str] = set()
    total_marks_all = 0
    per_book: List[Dict[str, int]] = []

    for path in book_files:
        try:
            content = load_book_content(path)
            uniq, marks = extract_words_from_content(content)
            total_marks_all += marks
            covered_words |= uniq
            per_book.append(
                {
                    "file": os.path.basename(path),
                    "unique_words": len(uniq),
                    "marks": marks,
                }
            )
        except Exception as e:
            print(f"âš ï¸  è·³è¿‡æ–‡ä»¶ï¼ˆè§£æå¤±è´¥ï¼‰ï¼š{os.path.basename(path)} -> {e}")

    covered_in_source = covered_words & source_set
    missing = sorted(list(source_set - covered_in_source))

    total_source = len(source_set)
    covered_count = len(covered_in_source)
    coverage_pct = (covered_count / total_source * 100) if total_source else 0.0

    print(f"\nğŸ“¦ å·²ä¸Šæ¶ä¹¦ç±æ•°ï¼š{len(book_files)}")
    print(f"ğŸ”– æ€»æ ‡è®°æ¬¡æ•°ï¼ˆ{ '{word|meaning}' }ï¼‰ï¼š{total_marks_all}")
    print(f"âœ… å»é‡åå·²å‡ºç°è‹±æ–‡è¯ï¼š{len(covered_words)}")
    print(f"ğŸ¯ å‘½ä¸­ IELTS æ ¸å¿ƒè¯ï¼š{covered_count} / {total_source}ï¼ˆ{coverage_pct:.2f}%ï¼‰")
    print(f"âŒ IELTS æ ¸å¿ƒè¯ç¼ºå¤±ï¼š{len(missing)}")

    # å†™ç¼ºå¤±è¯æ¸…å•ï¼ˆä¾¿äºåç»­è¡¥æ¼ï¼‰
    try:
        with open(OUT_MISSING_TXT, "w", encoding="utf-8") as f:
            for w in missing:
                f.write(w + "\n")
        print(f"\nğŸ“ ç¼ºå¤±è¯æ¸…å•å·²å†™å…¥ï¼š{OUT_MISSING_TXT}")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•å†™å…¥ç¼ºå¤±è¯æ–‡ä»¶ï¼š{e}")

    # å†™æ‘˜è¦ JSONï¼ˆç•™æ¡£ï¼‰
    try:
        summary = {
            "source_total": total_source,
            "books_total": len(book_files),
            "total_marks": total_marks_all,
            "covered_unique_total": len(covered_words),
            "covered_in_source": covered_count,
            "coverage_percent": round(coverage_pct, 4),
            "missing_count": len(missing),
            "missing_words_file": os.path.basename(OUT_MISSING_TXT),
            "per_book": per_book,
        }
        with open(OUT_SUMMARY_JSON, "w", encoding="utf-8") as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"ğŸ§¾ è¦†ç›–ç‡æ‘˜è¦å·²å†™å…¥ï¼š{OUT_SUMMARY_JSON}")
    except Exception as e:
        print(f"âš ï¸  æ— æ³•å†™å…¥æ‘˜è¦æ–‡ä»¶ï¼š{e}")

    # é«˜ä¿¡å·æç¤ºï¼šå¦‚æœæ²¡æ»¡ï¼Œç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®
    if missing:
        print("\nä¸‹ä¸€æ­¥å»ºè®®ï¼š")
        print("- ä½ å¯ä»¥æŠŠ missing_ielts_words.txt æŒ‰æ‰¹æ¬¡å–‚ç»™æ¨¡å‹ï¼Œç”Ÿæˆæ–°çš„å°è¯´æˆ–è¡¥å†™çŸ­æ®µè½æ¥è¦†ç›–ç¼ºå¤±è¯ã€‚")
        print("- æˆ–è€…åœ¨ step1 é€‰è¯é˜¶æ®µï¼Œä¼˜å…ˆä»ç¼ºå¤±è¯é‡ŒæŠ½å–ï¼Œç¡®ä¿æ¯ç¯‡éƒ½åœ¨è¡¥æ¼ã€‚")


if __name__ == "__main__":
    main()

