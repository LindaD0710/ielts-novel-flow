#!/usr/bin/env python3
"""
IELTS Novel Flow - è¯æ±‡æ‰¹æ¬¡ç”Ÿæˆå™¨

åŠŸèƒ½ï¼šä» ielts_source.json ä¸­æ‰¾å‡ºè¿˜æœªå½•å…¥ vocab_db.json çš„å•è¯ï¼Œ
      æŒ‰æ‰¹æ¬¡è¾“å‡ºï¼Œæ–¹ä¾¿å¤åˆ¶ç²˜è´´ç»™ ChatGPT/Gemini ç”Ÿæˆè¯æ±‡è¯¦æƒ…ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python3 vocab_batch_generator.py [æ‰¹æ¬¡å¤§å°] [æ‰¹æ¬¡ç¼–å·]
    
ç¤ºä¾‹ï¼š
    python3 vocab_batch_generator.py 50        # è¾“å‡ºç¬¬ä¸€æ‰¹ 50 ä¸ªæœªç”Ÿæˆçš„å•è¯
    python3 vocab_batch_generator.py 50 2      # è¾“å‡ºç¬¬äºŒæ‰¹ 50 ä¸ªæœªç”Ÿæˆçš„å•è¯
"""

import json
import os
import sys
from typing import List, Dict, Any

# ==================== è·¯å¾„é…ç½® ====================
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(BASE_DIR)

SOURCE_FILE = os.path.join(BASE_DIR, "ielts_source.json")
VOCAB_DB_FILE = os.path.join(PROJECT_ROOT, "src", "data", "generated", "vocab_db.json")


def load_source_words() -> List[str]:
    """åŠ è½½è¯æºåˆ—è¡¨"""
    if not os.path.exists(SOURCE_FILE):
        print(f"âŒ é”™è¯¯ï¼šè¯æºæ–‡ä»¶ä¸å­˜åœ¨ï¼š{SOURCE_FILE}")
        sys.exit(1)
    
    with open(SOURCE_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    if not isinstance(data, list):
        print(f"âŒ é”™è¯¯ï¼šè¯æºæ–‡ä»¶å¿…é¡»æ˜¯å­—ç¬¦ä¸²æ•°ç»„")
        sys.exit(1)
    
    words = [w.strip() for w in data if isinstance(w, str) and w.strip()]
    return words


def load_vocab_db() -> Dict[str, Any]:
    """åŠ è½½å·²æœ‰çš„è¯æ±‡æ•°æ®åº“"""
    if not os.path.exists(VOCAB_DB_FILE):
        return {}
    
    with open(VOCAB_DB_FILE, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            return {}
    
    if not isinstance(data, dict):
        return {}
    
    # ç»Ÿä¸€è½¬ä¸ºå°å†™ key
    normalized = {}
    for k, v in data.items():
        if isinstance(k, str):
            normalized[k.lower()] = v
    return normalized


def get_missing_words(source_words: List[str], vocab_db: Dict[str, Any]) -> List[str]:
    """æ‰¾å‡ºè¿˜æœªå½•å…¥çš„å•è¯"""
    missing = []
    for word in source_words:
        key = word.lower()
        if key not in vocab_db:
            missing.append(word)
    return missing


def format_words_for_chat(words: List[str], format_type: str = "comma") -> str:
    """
    æ ¼å¼åŒ–å•è¯åˆ—è¡¨ï¼Œæ–¹ä¾¿å¤åˆ¶ç²˜è´´ç»™ ChatGPT/Gemini
    
    Args:
        words: å•è¯åˆ—è¡¨
        format_type: æ ¼å¼ç±»å‹
            - "comma": é€—å·åˆ†éš”ï¼ˆä¸€è¡Œï¼‰
            - "comma_space": é€—å·+ç©ºæ ¼åˆ†éš”ï¼ˆä¸€è¡Œï¼‰
            - "newline": æ¯è¡Œä¸€ä¸ªå•è¯
            - "numbered": å¸¦ç¼–å·çš„åˆ—è¡¨
    """
    if format_type == "comma":
        return ", ".join(words)
    elif format_type == "comma_space":
        return ", ".join(words)
    elif format_type == "newline":
        return "\n".join(words)
    elif format_type == "numbered":
        return "\n".join([f"{i+1}. {word}" for i, word in enumerate(words)])
    else:
        return ", ".join(words)


def get_next_batch_number(batch_size: int) -> int:
    """
    è‡ªåŠ¨æ£€æµ‹ä¸‹ä¸€ä¸ªæ‰¹æ¬¡ç¼–å·
    é€šè¿‡æ£€æŸ¥å·²å­˜åœ¨çš„æ‰¹æ¬¡æ–‡ä»¶ï¼Œæ‰¾åˆ°æœ€å¤§çš„ç¼–å·ï¼Œç„¶å+1
    """
    import glob
    pattern = os.path.join(BASE_DIR, "vocab_batch_*.json")
    existing_files = glob.glob(pattern)
    
    max_num = 0
    for filepath in existing_files:
        filename = os.path.basename(filepath)
        # æå–ç¼–å·ï¼švocab_batch_001.json -> 1
        try:
            num_str = filename.replace("vocab_batch_", "").replace(".json", "")
            num = int(num_str)
            if num > max_num:
                max_num = num
        except ValueError:
            continue
    
    return max_num + 1


def main():
    print("=" * 60)
    print("IELTS Novel Flow - è¯æ±‡æ‰¹æ¬¡ç”Ÿæˆå™¨")
    print("=" * 60)
    print()
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    batch_size = 50  # é»˜è®¤æ¯æ‰¹ 50 ä¸ª
    batch_num = None  # å¦‚æœæœªæŒ‡å®šï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹
    
    if len(sys.argv) > 1:
        try:
            batch_size = int(sys.argv[1])
        except ValueError:
            print(f"âŒ é”™è¯¯ï¼šæ‰¹æ¬¡å¤§å°å¿…é¡»æ˜¯æ•°å­—ï¼Œä½ è¾“å…¥çš„æ˜¯ï¼š{sys.argv[1]}")
            sys.exit(1)
    
    if len(sys.argv) > 2:
        try:
            batch_num = int(sys.argv[2])
        except ValueError:
            print(f"âŒ é”™è¯¯ï¼šæ‰¹æ¬¡ç¼–å·å¿…é¡»æ˜¯æ•°å­—ï¼Œä½ è¾“å…¥çš„æ˜¯ï¼š{sys.argv[2]}")
            sys.exit(1)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“– åŠ è½½è¯æºåˆ—è¡¨...")
    source_words = load_source_words()
    print(f"âœ… è¯æºæ€»æ•°ï¼š{len(source_words)}")
    
    print("\nğŸ“š åŠ è½½å·²æœ‰è¯æ±‡åº“...")
    vocab_db = load_vocab_db()
    print(f"âœ… å·²å½•å…¥ï¼š{len(vocab_db)} ä¸ªå•è¯")
    
    # æ‰¾å‡ºç¼ºå¤±çš„å•è¯
    print("\nğŸ” æŸ¥æ‰¾æœªå½•å…¥çš„å•è¯...")
    missing_words = get_missing_words(source_words, vocab_db)
    print(f"âœ… å¾…ç”Ÿæˆï¼š{len(missing_words)} ä¸ªå•è¯")
    
    if not missing_words:
        print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰å•è¯éƒ½å·²å½•å…¥ï¼Œvocab_db.json å·²å®Œæ•´ã€‚")
        return
    
    # è®¡ç®—å®é™…æ‰¹æ¬¡ï¼ˆåŸºäºå¾…ç”Ÿæˆçš„å•è¯ï¼‰
    total_batches = (len(missing_words) + batch_size - 1) // batch_size
    
    # å¦‚æœæœªæŒ‡å®šæ‰¹æ¬¡ç¼–å·ï¼Œè‡ªåŠ¨æ£€æµ‹ä¸‹ä¸€ä¸ªæ–‡ä»¶ç¼–å·
    if batch_num is None:
        file_batch_num = get_next_batch_number(batch_size)
        print(f"\nğŸ’¡ è‡ªåŠ¨æ£€æµ‹ï¼šä¸‹ä¸€ä¸ªæ–‡ä»¶ç¼–å·ä¸º {file_batch_num}")
        # æ‰¹æ¬¡å†…å®¹ä»ç¬¬1æ‰¹å¼€å§‹ï¼ˆåŸºäºå¾…ç”Ÿæˆçš„å•è¯ï¼‰
        content_batch_num = 1
    else:
        # ç”¨æˆ·æŒ‡å®šäº†æ‰¹æ¬¡ç¼–å·ï¼Œç”¨äºæ–‡ä»¶å‘½å
        file_batch_num = batch_num
        # è®¡ç®—è¿™æ˜¯ç¬¬å‡ ä¸ª"å¾…ç”Ÿæˆæ‰¹æ¬¡"
        # å¦‚æœç”¨æˆ·ä¹‹å‰å·²ç»å¯¼å…¥äº†å¾ˆå¤šæ‰¹æ¬¡ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—è¿™æ˜¯ç¬¬å‡ ä¸ªå¾…ç”Ÿæˆçš„æ‰¹æ¬¡
        # ç®€å•æ–¹å¼ï¼šå‡è®¾ç”¨æˆ·æƒ³ä»ç¬¬1ä¸ªå¾…ç”Ÿæˆçš„æ‰¹æ¬¡å¼€å§‹
        content_batch_num = 1
        print(f"\nğŸ’¡ ä½¿ç”¨æ–‡ä»¶ç¼–å·ï¼š{file_batch_num}ï¼Œå†…å®¹æ‰¹æ¬¡ï¼š{content_batch_num}")
    
    # è®¡ç®—æ‰¹æ¬¡å†…å®¹ï¼ˆåŸºäºå¾…ç”Ÿæˆçš„å•è¯ï¼Œä»ç¬¬1æ‰¹å¼€å§‹ï¼‰
    start_idx = (content_batch_num - 1) * batch_size
    end_idx = min(start_idx + batch_size, len(missing_words))
    
    if start_idx >= len(missing_words):
        print(f"\nâŒ é”™è¯¯ï¼šæ²¡æœ‰æ›´å¤šå¾…ç”Ÿæˆçš„å•è¯äº†ï¼ˆæ€»å…±åªæœ‰ {total_batches} æ‰¹ï¼‰")
        return
    
    # ä½¿ç”¨æ–‡ä»¶ç¼–å·ç”¨äºæ˜¾ç¤ºå’Œæ–‡ä»¶å‘½å
    batch_num = file_batch_num
    
    batch_words = missing_words[start_idx:end_idx]
    
    print(f"\nğŸ“¦ æ–‡ä»¶ç¼–å·ï¼švocab_batch_{batch_num:03d}.json")
    print(f"   å†…å®¹æ‰¹æ¬¡ï¼šç¬¬ {content_batch_num} æ‰¹ / å…± {total_batches} æ‰¹ï¼ˆåŸºäºå¾…ç”Ÿæˆçš„å•è¯ï¼‰")
    print(f"   å•è¯èŒƒå›´ï¼šç¬¬ {start_idx + 1} - {end_idx} ä¸ªï¼ˆå…± {len(batch_words)} ä¸ªï¼‰")
    print()
    print("=" * 60)
    print("ğŸ“‹ å•è¯åˆ—è¡¨ï¼ˆå¯ç›´æ¥å¤åˆ¶ç»™ ChatGPT/Geminiï¼‰ï¼š")
    print("=" * 60)
    print()
    
    # è¾“å‡ºæ ¼å¼1ï¼šé€—å·åˆ†éš”ï¼ˆæœ€å¸¸ç”¨ï¼‰
    print("ã€æ ¼å¼1ï¼šé€—å·åˆ†éš”ï¼ˆæ¨èï¼‰ã€‘")
    print(format_words_for_chat(batch_words, "comma"))
    print()
    
    # è¾“å‡ºæ ¼å¼2ï¼šæ¯è¡Œä¸€ä¸ªï¼ˆå¤‡ç”¨ï¼‰
    print("=" * 60)
    print("ã€æ ¼å¼2ï¼šæ¯è¡Œä¸€ä¸ªï¼ˆå¤‡ç”¨ï¼‰ã€‘")
    print(format_words_for_chat(batch_words, "newline"))
    print()
    
    print("=" * 60)
    print("\nğŸ’¡ ä½¿ç”¨æç¤ºï¼š")
    print(f"   1. å¤åˆ¶ä¸Šé¢çš„å•è¯åˆ—è¡¨ï¼ˆæ¨èç”¨æ ¼å¼1ï¼‰")
    print(f"   2. åœ¨ ChatGPT/Gemini ä¸­ç”Ÿæˆè¯æ±‡è¯¦æƒ… JSON")
    print(f"   3. ä¿å­˜ä¸ºæ–‡ä»¶ï¼švocab_batch_{batch_num:03d}.json")
    print(f"   4. è¿è¡Œï¼špython3 vocab_manual_import.py vocab_batch_{batch_num:03d}.json")
    print()
    if content_batch_num < total_batches:
        next_file_num = batch_num + 1
        print(f"   5. ç”Ÿæˆä¸‹ä¸€æ‰¹ï¼ˆè‡ªåŠ¨ç¼–å·ï¼‰ï¼špython3 vocab_batch_generator.py {batch_size}")
        print(f"      ä¸‹ä¸€æ‰¹æ–‡ä»¶å°†ä¿å­˜ä¸ºï¼švocab_batch_{next_file_num:03d}.json")
        print(f"      ï¼ˆè¿˜æœ‰ {total_batches - content_batch_num} æ‰¹å¾…ç”Ÿæˆï¼‰")
    else:
        print(f"   âœ… è¿™æ˜¯æœ€åä¸€æ‰¹äº†ï¼æ‰€æœ‰å¾…ç”Ÿæˆçš„å•è¯éƒ½å·²å®Œæˆã€‚")
    print()


if __name__ == "__main__":
    main()

