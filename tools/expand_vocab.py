#!/usr/bin/env python3
"""
è¯åº“æ‰©å……å·¥å…·

åŠŸèƒ½ï¼šå°†æ–‡æœ¬æ ¼å¼çš„è¯åº“æ–‡ä»¶å¯¼å…¥åˆ° ielts_source.json
"""

import json
import os
import sys
from typing import List

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
IELTS_SOURCE_FILE = os.path.join(BASE_DIR, "ielts_source.json")


def load_existing_words() -> List[str]:
    """åŠ è½½ç°æœ‰è¯åº“"""
    if not os.path.exists(IELTS_SOURCE_FILE):
        return []
    
    try:
        with open(IELTS_SOURCE_FILE, 'r', encoding='utf-8') as f:
            words = json.load(f)
        return words if isinstance(words, list) else []
    except:
        return []


def load_words_from_text_file(text_file: str) -> List[str]:
    """ä»æ–‡æœ¬æ–‡ä»¶è¯»å–å•è¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰"""
    words = []
    with open(text_file, 'r', encoding='utf-8') as f:
        for line in f:
            word = line.strip().lower()
            if word and word.isalpha():  # åªä¿ç•™å­—æ¯å•è¯
                words.append(word)
    return words


def expand_vocab_from_text(text_file: str):
    """ä»æ–‡æœ¬æ–‡ä»¶æ‰©å……è¯åº“"""
    print("=" * 60)
    print("ğŸ“š è¯åº“æ‰©å……å·¥å…·")
    print("=" * 60)
    print()
    
    # åŠ è½½ç°æœ‰è¯åº“
    existing_words = load_existing_words()
    print(f"ğŸ“Š å½“å‰è¯åº“ï¼š{len(existing_words)} ä¸ªå•è¯")
    
    # è¯»å–æ–°å•è¯
    if not os.path.exists(text_file):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼š{text_file}")
        sys.exit(1)
    
    print(f"ğŸ“– è¯»å–æ–‡ä»¶ï¼š{text_file}")
    new_words = load_words_from_text_file(text_file)
    print(f"ğŸ“Š æ–°å•è¯ï¼š{len(new_words)} ä¸ª")
    
    # åˆå¹¶å¹¶å»é‡
    all_words = list(set(existing_words + new_words))
    all_words.sort()  # æ’åº
    
    # ä¿å­˜
    with open(IELTS_SOURCE_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_words, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("âœ… æ‰©å……å®Œæˆï¼")
    print("=" * 60)
    print(f"æ‰©å……å‰ï¼š{len(existing_words)} ä¸ª")
    print(f"æ–°å¢ï¼š{len(new_words)} ä¸ª")
    print(f"æ‰©å……åï¼š{len(all_words)} ä¸ªï¼ˆå»é‡åï¼‰")
    print(f"ç›®æ ‡ï¼š4000 ä¸ª")
    
    if len(all_words) >= 4000:
        print("âœ… å·²è¾¾åˆ°ç›®æ ‡ï¼")
    else:
        print(f"âš ï¸  è¿˜éœ€ {4000 - len(all_words)} ä¸ªå•è¯")
    print("=" * 60)


def expand_vocab_manually(words: List[str]):
    """æ‰‹åŠ¨æ·»åŠ å•è¯åˆ—è¡¨"""
    print("=" * 60)
    print("ğŸ“š è¯åº“æ‰©å……å·¥å…·ï¼ˆæ‰‹åŠ¨æ·»åŠ ï¼‰")
    print("=" * 60)
    print()
    
    # åŠ è½½ç°æœ‰è¯åº“
    existing_words = load_existing_words()
    print(f"ğŸ“Š å½“å‰è¯åº“ï¼š{len(existing_words)} ä¸ªå•è¯")
    
    # å¤„ç†æ–°å•è¯
    new_words = [w.strip().lower() for w in words if w.strip() and w.strip().isalpha()]
    print(f"ğŸ“Š æ–°å¢å•è¯ï¼š{len(new_words)} ä¸ª")
    
    # åˆå¹¶å¹¶å»é‡
    all_words = list(set(existing_words + new_words))
    all_words.sort()
    
    # ä¿å­˜
    with open(IELTS_SOURCE_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_words, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("âœ… æ‰©å……å®Œæˆï¼")
    print("=" * 60)
    print(f"æ‰©å……å‰ï¼š{len(existing_words)} ä¸ª")
    print(f"æ–°å¢ï¼š{len(new_words)} ä¸ª")
    print(f"æ‰©å……åï¼š{len(all_words)} ä¸ªï¼ˆå»é‡åï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼š")
        print("  python3 expand_vocab.py <text_file>")
        print("")
        print("è¯´æ˜ï¼š")
        print("  text_file: æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯è¡Œä¸€ä¸ªå•è¯")
        print("")
        print("ç¤ºä¾‹ï¼š")
        print("  python3 expand_vocab.py vocab_list.txt")
        sys.exit(1)
    
    text_file = sys.argv[1]
    expand_vocab_from_text(text_file)

