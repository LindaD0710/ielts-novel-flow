#!/usr/bin/env python3
"""
ä» GitHub é¡¹ç›®æå–é›…æ€è¯æ±‡

ä» https://github.com/hefengxian/my-ielts é¡¹ç›®æå–è¯æ±‡åˆ—è¡¨
"""

import json
import os
import re
import requests
from typing import Set, List

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
IELTS_SOURCE_FILE = os.path.join(BASE_DIR, "ielts_source.json")
GITHUB_REPO = "hefengxian/my-ielts"
GITHUB_BRANCH = "master"


def load_existing_words() -> Set[str]:
    """åŠ è½½ç°æœ‰è¯åº“"""
    if not os.path.exists(IELTS_SOURCE_FILE):
        return set()
    
    try:
        with open(IELTS_SOURCE_FILE, 'r', encoding='utf-8') as f:
            words = json.load(f)
        return set(w.lower() for w in words if w.strip())
    except:
        return set()


def get_github_file_tree(path: str) -> List[dict]:
    """è·å–GitHubä»“åº“çš„ç›®å½•æ ‘"""
    url = f"https://api.github.com/repos/{GITHUB_REPO}/git/trees/{GITHUB_BRANCH}?recursive=1"
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        data = response.json()
        return data.get('tree', [])
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•ä»GitHubè·å–æ–‡ä»¶åˆ—è¡¨ï¼š{e}")
        return []


def extract_words_from_paths(file_paths: List[str]) -> Set[str]:
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–å•è¯"""
    words = set()
    
    for path in file_paths:
        # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åå’Œç›®å½•ï¼‰
        filename = os.path.basename(path)
        # ç§»é™¤æ‰©å±•å
        name_without_ext = os.path.splitext(filename)[0]
        
        # è·³è¿‡éå•è¯æ–‡ä»¶åï¼ˆåŒ…å«ä¸­æ–‡å­—ç¬¦ã€ç‰¹æ®Šå­—ç¬¦ç­‰ï¼‰
        if re.match(r'^[a-zA-Z][a-zA-Z\s]*[a-zA-Z]$', name_without_ext) or re.match(r'^[a-zA-Z]+$', name_without_ext):
            # å¤„ç†å¯èƒ½åŒ…å«å¤šä¸ªå•è¯çš„æƒ…å†µï¼ˆç”¨ç©ºæ ¼æˆ–ä¸‹åˆ’çº¿åˆ†éš”ï¼‰
            potential_words = re.split(r'[\s_\-]+', name_without_ext)
            for word in potential_words:
                word = word.strip()
                # åªä¿ç•™çœ‹èµ·æ¥åƒè‹±æ–‡å•è¯çš„ï¼ˆè‡³å°‘2ä¸ªå­—æ¯ï¼ŒåªåŒ…å«å­—æ¯ï¼‰
                if len(word) >= 2 and word.isalpha() and word.islower():
                    words.add(word.lower())
                elif len(word) >= 2 and word.isalpha():
                    # å¤„ç†é¦–å­—æ¯å¤§å†™çš„å•è¯ï¼ˆå¦‚ä¸“æœ‰åè¯ï¼‰
                    # åªæå–å…¨å°å†™çš„å•è¯ï¼Œé¦–å­—æ¯å¤§å†™çš„å¯èƒ½æ˜¯ä¸“æœ‰åè¯
                    if word[0].isupper() and word[1:].islower():
                        words.add(word.lower())
    
    return words


def extract_from_github():
    """ä»GitHubä»“åº“æå–è¯æ±‡"""
    print("=" * 60)
    print("ğŸ“š ä» GitHub æå–é›…æ€è¯æ±‡")
    print("=" * 60)
    print(f"ä»“åº“ï¼š{GITHUB_REPO}")
    print(f"åˆ†æ”¯ï¼š{GITHUB_BRANCH}")
    print()
    
    # è·å–æ–‡ä»¶æ ‘
    print("ğŸ“¡ æ­£åœ¨è·å–æ–‡ä»¶åˆ—è¡¨...")
    tree = get_github_file_tree("")
    
    if not tree:
        print("âŒ æ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨")
        return
    
    # ç­›é€‰è¯æ±‡ç›¸å…³çš„æ–‡ä»¶
    print("ğŸ” æ­£åœ¨ç­›é€‰è¯æ±‡æ–‡ä»¶...")
    vocab_paths = []
    for item in tree:
        path = item.get('path', '')
        # æŸ¥æ‰¾ vocabulary ç›¸å…³çš„æ–‡ä»¶
        if 'vocabulary' in path.lower() or 'vocab' in path.lower():
            # éŸ³é¢‘æ–‡ä»¶æˆ–JSONæ–‡ä»¶
            if path.endswith('.mp3') or path.endswith('.json') or path.endswith('.txt'):
                vocab_paths.append(path)
    
    print(f"âœ… æ‰¾åˆ° {len(vocab_paths)} ä¸ªç›¸å…³æ–‡ä»¶")
    
    # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–å•è¯
    print("ğŸ“ æ­£åœ¨æå–å•è¯...")
    extracted_words = extract_words_from_paths(vocab_paths)
    print(f"âœ… æå–åˆ° {len(extracted_words)} ä¸ªå•è¯")
    
    # åŠ è½½ç°æœ‰è¯åº“
    existing_words = load_existing_words()
    print(f"ğŸ“Š ç°æœ‰è¯åº“ï¼š{len(existing_words)} ä¸ªå•è¯")
    
    # åˆå¹¶
    all_words = existing_words | extracted_words
    all_words_list = sorted(list(all_words))
    
    # ä¿å­˜
    with open(IELTS_SOURCE_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_words_list, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("âœ… æå–å®Œæˆï¼")
    print("=" * 60)
    print(f"æå–çš„æ–°å•è¯ï¼š{len(extracted_words)} ä¸ª")
    print(f"æ‰©å……å‰ï¼š{len(existing_words)} ä¸ª")
    print(f"æ‰©å……åï¼š{len(all_words_list)} ä¸ªï¼ˆå»é‡åï¼‰")
    print(f"ç›®æ ‡ï¼š4000 ä¸ª")
    
    if len(all_words_list) >= 4000:
        print("âœ… å·²è¾¾åˆ°ç›®æ ‡ï¼")
    else:
        print(f"âš ï¸  è¿˜éœ€ {4000 - len(all_words_list)} ä¸ªå•è¯")
    
    print()
    print("ğŸ“‹ æå–çš„å•è¯ç¤ºä¾‹ï¼ˆå‰20ä¸ªï¼‰ï¼š")
    for i, word in enumerate(all_words_list[:20], 1):
        print(f"   {i:2}. {word}")
    if len(all_words_list) > 20:
        print(f"   ... è¿˜æœ‰ {len(all_words_list) - 20} ä¸ªå•è¯")
    
    print("=" * 60)


def extract_from_github_raw():
    """ç›´æ¥ä»GitHub rawæ–‡ä»¶æå–ï¼ˆæ›´ç›´æ¥çš„æ–¹æ³•ï¼‰"""
    print("=" * 60)
    print("ğŸ“š ä» GitHub Raw æ–‡ä»¶æå–è¯æ±‡")
    print("=" * 60)
    print()
    
    # å°è¯•ä»å·²çŸ¥çš„è·¯å¾„è·å–è¯æ±‡æ–‡ä»¶
    # é€šå¸¸è¯æ±‡å¯èƒ½åœ¨ JSON æˆ–æ–‡æœ¬æ–‡ä»¶ä¸­
    possible_paths = [
        "public/vocabulary/data.json",
        "src/data/vocabulary.json",
        "src/vocabulary.json",
        "vocabulary.json",
        "data/vocabulary.json",
    ]
    
    extracted_words = set()
    
    for path in possible_paths:
        url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/{GITHUB_BRANCH}/{path}"
        try:
            print(f"ğŸ“¡ å°è¯•è·å–ï¼š{path}")
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                data = response.json()
                # å°è¯•ä¸åŒçš„JSONç»“æ„
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, str):
                            extracted_words.add(item.lower())
                        elif isinstance(item, dict):
                            # æŸ¥æ‰¾å¯èƒ½çš„å•è¯å­—æ®µ
                            for key in ['word', 'name', 'vocab', 'term']:
                                if key in item and isinstance(item[key], str):
                                    extracted_words.add(item[key].lower())
                elif isinstance(data, dict):
                    # å¯èƒ½æ˜¯å¯¹è±¡ï¼Œé”®æˆ–å€¼å¯èƒ½æ˜¯å•è¯
                    for key, value in data.items():
                        if isinstance(key, str) and key.isalpha():
                            extracted_words.add(key.lower())
                        if isinstance(value, str) and value.isalpha():
                            extracted_words.add(value.lower())
                print(f"âœ… æˆåŠŸä» {path} æå– {len(extracted_words)} ä¸ªå•è¯")
                break
        except:
            continue
    
    if not extracted_words:
        print("âš ï¸  æ— æ³•ä»æ ‡å‡†è·¯å¾„è·å–ï¼Œå°è¯•ä»æ–‡ä»¶æ ‘æå–...")
        extract_from_github()
        return
    
    # åŠ è½½ç°æœ‰è¯åº“
    existing_words = load_existing_words()
    print(f"ğŸ“Š ç°æœ‰è¯åº“ï¼š{len(existing_words)} ä¸ªå•è¯")
    
    # åˆå¹¶
    all_words = existing_words | extracted_words
    all_words_list = sorted(list(all_words))
    
    # ä¿å­˜
    with open(IELTS_SOURCE_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_words_list, f, ensure_ascii=False, indent=2)
    
    print()
    print("=" * 60)
    print("âœ… æå–å®Œæˆï¼")
    print("=" * 60)
    print(f"æ‰©å……åï¼š{len(all_words_list)} ä¸ªï¼ˆå»é‡åï¼‰")
    print("=" * 60)


if __name__ == "__main__":
    try:
        # å…ˆå°è¯•ä» raw æ–‡ä»¶æå–ï¼ˆæ›´å¿«ï¼‰
        extract_from_github_raw()
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()

